# =========================================================================
# Copyright (C) 2025. FuxiCTR Authors. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# =========================================================================


import argparse
import yaml
import os
import shutil
from pathlib import Path
import fuxictr_version
from fuxictr import autotuner

yaml.Dumper.ignore_aliases = lambda *args: True


def load_multi_experiment_config(config_path):
    """Load multi-experiment configuration from YAML file."""
    with open(config_path, 'r') as f:
        return yaml.load(f, Loader=yaml.FullLoader)


def generate_tuner_config(dataset_name, model_name, max_seq_len, multi_config):
    """
    Generate tuner config dict for a single experiment combination.

    Args:
        dataset_name: Name of the dataset (e.g., 'redkiwi_1')
        model_name: Name of the model (e.g., 'mirrn', 'twin')
        max_seq_len: Maximum sequence length (e.g., 500)
        multi_config: Multi-experiment configuration dict

    Returns:
        tuple: (tuner_config dict, dataset_id string)
    """
    model_config = multi_config['model_base_configs'][model_name]
    dataset_config = multi_config['datasets'][dataset_name]
    tuner_space = multi_config['tuner_spaces'][model_name]

    data_root = multi_config['data_root_template'].format(
        dataset_name=dataset_name,
        max_user_seq_len=max_seq_len
    )

    dataset_id = f"{dataset_name}_maxlen{max_seq_len}"

    tuner_config = {
        'base_config': model_config['base_config'],
        'base_expid': model_config['base_expid'],
        'dataset_id': dataset_id,
        'dataset_config': {
            dataset_id: {
                'data_root': data_root,
                'data_format': dataset_config['data_format'],
                'train_data': f"{data_root}train_longctr.parquet",
                'valid_data': f"{data_root}valid_longctr.parquet",
                'test_data': f"{data_root}test_longctr.parquet",
                'user_info': f"{data_root}user_info.parquet",
                'item_info': f"{data_root}item_info.parquet",
                'rebuild_dataset': dataset_config['rebuild_dataset'],
                'feature_cols': dataset_config['feature_cols'],
                'label_col': dataset_config['label_col']
            }
        },
        'tuner_space': tuner_space
    }

    return tuner_config, dataset_id


def run_single_experiment(dataset, model, max_seq_len, multi_config, gpu_list):
    """
    Run a single experiment for one (dataset, model, max_seq_len) combination.

    Args:
        dataset: Dataset name
        model: Model name
        max_seq_len: Maximum sequence length
        multi_config: Multi-experiment configuration dict
        gpu_list: List of GPU IDs
    """
    tuner_config, dataset_id = generate_tuner_config(
        dataset, model, max_seq_len, multi_config
    )

    temp_config_path = f"./config/temp_config/temp_tuner_{dataset}_{model}_{max_seq_len}.yaml"
    os.makedirs("./config/temp_config", exist_ok=True)

    with open(temp_config_path, 'w') as f:
        yaml.dump(tuner_config, f, default_flow_style=None, indent=4)

    config_dir = autotuner.enumerate_params(temp_config_path)

    autotuner.grid_search(config_dir, gpu_list)

    results_dir = multi_config['results_root_template'].format(
        dataset_name=dataset,
        max_user_seq_len=max_seq_len
    )
    os.makedirs(results_dir, exist_ok=True)

    # CSV filename is based on config_dir name (generated by run_expid.py)
    config_dir_name = os.path.basename(config_dir)
    source_csv = f"{config_dir_name}.csv"
    dest_csv = os.path.join(results_dir, f"tuner_{dataset}_{model}_{max_seq_len}.csv")

    if os.path.exists(source_csv):
        shutil.move(source_csv, dest_csv)
        print(f"Results saved to: {dest_csv}")
    else:
        print(f"Warning: Results CSV not found: {source_csv}")

    if os.path.exists(temp_config_path):
        os.remove(temp_config_path)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str,
                       default='./config/multi_experiment_config.yaml',
                       help='Path to multi-experiment configuration file')
    parser.add_argument('--gpu', nargs='+', default=[-1],
                       help='List of GPU devices, -1 for CPU')
    args = vars(parser.parse_args())

    multi_config = load_multi_experiment_config(args['config'])
    gpu_list = args['gpu']

    datasets = multi_config['dataset_list']
    models = multi_config['model_list']
    max_seq_lens = multi_config['max_user_seq_len_list']

    total = len(datasets) * len(models) * len(max_seq_lens)
    count = 0

    print(f"Starting multi-experiment run: {total} total experiments")
    print(f"Datasets: {datasets}")
    print(f"Models: {models}")
    print(f"Max sequence lengths: {max_seq_lens}")
    print("=" * 80)

    for dataset in datasets:
        for model in models:
            for max_seq_len in max_seq_lens:
                count += 1
                print(f"\n[{count}/{total}] Running: {dataset} + {model} + maxlen{max_seq_len}")
                print("-" * 80)
                run_single_experiment(dataset, model, max_seq_len, multi_config, gpu_list)

    print("\n" + "=" * 80)
    print(f"All {total} experiments completed!")


if __name__ == '__main__':
    main()
